created: 20170317104352496
modified: 20170612032557284
tags: [[Optimization Algorithms]] [[Reinforcement Learning]]
title: Overcoming catastrophic forgetting in NNs
type: text/vnd.tiddlywiki

! EWC

[[link|http://www.pnas.org/content/early/2017/03/13/1611835114.abstract]]

Elastic weight consolidation is best described as online sequential (diagonalised) Laplace approximation. This is similar to [[assumed density filtering|http://publications.aston.ac.uk/1444/1/NCRG_99_029.pdf]] the precursor to expectation-propagation. Laplace approximation takes a probability density $p$ and approximates it with a Gaussian. By Bernstein-von Misestype convergence theorems, the posterior will converge to the Laplace. EWC use a diagonalised Hessian for variance estimation.

For the first task, the Hessian would be the Fisher information from task $A$, $F^A$, plus the Hessian of the log prior $\log p(\theta)$.

Bayesian inference wouldn't suffer from catastrophic forgetting which haunts optimization-based methods. But the full posterior is intractable. 

* EWC approximated Bayesian computation

The problem is begins at the third task, the right approximation
$$
\log p(\theta|\mathcal D_A, \mathcal D_B)\approx -\sum_i(F^A+F^B)_{i, i}(\theta_i-\theta_i^{A, B})^2
$$

The penalty around $\theta^A$ has already been taken account.

EWC further required the knowledge of the task it is performing. The tasks here can be different minibatches of the same task.

! Generative Replay
Use a GAN to generate samples of previous tasks.