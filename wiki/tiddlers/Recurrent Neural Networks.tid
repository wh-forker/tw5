created: 20150514055814821
modified: 20171106233741005
tags: [[Deep Neural Networks Architectures]]
title: Recurrent Neural Networks
type: text/vnd.tiddlywiki

! Introduction

RNN is a very deep feedforward neural newwork that has a layer for each timestep. Its weights are shared across time.

! Model structure
* [[Sequential Models]]
* Regularisation
** don't put drop on recurrent connections, on non-recurrent ones. [[Recurrent neural network regularization. Zaremba et al.|https://arxiv.org/abs/1409.2329]]
** [[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. Gal and Ghahramani|https://arxiv.org/abs/1512.05287]] advocate tying the recurrent dropout mask and sampling at evaluation time.
** [[Variational Dropout]]

! Topics

* ICLR 2017 Talk: [[New Directions for RNNs]]
* [[Meta-Learning]]
* [[RNN Performance Optimization]]

! Training
* [[Optimizing RNN]]
* RTRL
** Approximate: [[Training recurrent networks online without backtracking|https://arxiv.org/abs/1507.07680]]
** Local: original LSTM paper
* [[Synthetic Gradients]]
* [[Waybackprop]]
* [[Training Recurrent Networks Online Without Backtracking]]

! Tools
* [[RNNVis|https://github.com/myaooo/RNNVis]] tf 0.12 visualization