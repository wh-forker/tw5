created: 20150310075400270
modified: 20150317081502675
tags: Bayesian
title: Dirichlet Process Mixtures
type: text/vnd.tiddlywiki

Let $H$ be a measure on some parameter space $\Theta$, like the [[conjugate priors|Dirichlet-multinomial distribution]], a Dirichlet process, , is then a distribution over measures on $\Theta$, where the ''scalar concentration parameter'' $\gamma$ controls the similarity of samples $G\sim DP(\gamma, H)$ to the base measure $H$. Analogously to Gaussian processes, DPs may be characterized by the distribution they induce on finite, measurable partitions $(T_1,\dots, T_l)$ of $\Theta$. For such partitions, the random vector $(G(T_1), \dots, G(T_l))$ has a finite-dimensional Dirichlet distribution:
$$
(G(T_1),\dots,G(T_l))\sim Dir(\gamma H(T_1), \dots,\gamma H(T_l))
$$
Samples from DPs are discrete with probability one, a property highlighted by the following ''stick-breaking construction'':
$$
G(\theta)=\sum_{k=1}^\infty\beta_k\delta(\theta, \theta_k) \qquad \beta'_k\sim Beta(1, \gamma) \qquad \beta_k=\beta'_k\prod_{l=1}^{k-1}(1-\beta'_l)
$$
Each parameter $\theta_k\sim H$ is independently sampled from the base measure, while the weights $\beta=(\beta_1, \beta_2,\dots)$ use beta random variables to partition a unit-length "stick" of probability mass. We denote $\beta\sim GEM(\gamma)$ a sample from this stick-breaking process.

<<<
As $\gamma$ becomes large, $E[\beta'_k]=1/(1+\gamma)\rightarrow0$, and $G$ approaches $H$ by uniformly distributing probability mass among a densely sampled set of discrete parameters $\{\theta_k\}^\infty_{k=1}$.
<<<

Given $G\sim DP(\gamma, H)$, each observation $x_i$ is generated by first choosing a parameter $\bar\theta_i\sim G$, and then sampling $x_i\sim F(\bar\theta_i)$. 

''Compute with CRP'': we let $z_i\sim\beta$ indicate the unique component of $G(\theta)$ associated with observation $x_i\sim F(\theta_{z_i})$. Marginalizing $G$, these assignments $z$ demonstrate an important clustering behavior. Letting $N_k$ denote the number of observations already assigned to $\theta_k$,
$$
p(z_i|z_{<i},\gamma) = \frac{1}{\gamma+i-1}\left[\sum_kN_k\delta(z_i, k)+\gamma\delta(z_i, \bar k)\right].
$$
Here, $\bar k$ indicates a previously unused mixture component.

! Sampling
The HDP follows an extension of the DP analogy known as the ''Chinese reataurant franchise''. Each object or group defines a separate restaurant in which customers (observed features) $(w_{ji}, v_{ji})$ sit at tables (clusters or parts) $t_{ji}$. Each table shares ''a single dish'' (parameter) $\tilde\theta_{lt}$, which is ordered from a menu $G_0$ shared among restaurants. Let $\mathbf k_l=\{k_{lt}\}$ denote the global parts assigned to all tables (local parts) of category $l$. We may then integrate over $G_0$ and $G_l$ to find the conditional distributions of these assignment variables:
$$
\begin{align*}
p(t_{ji}|t_{j\bar i}, \alpha)&\propto\sum_tN_{jt}\delta(t_{ji}, t) + \alpha\delta(t_{ji}, \bar t)\\
p(k_{lt}|\mathbf k_{\bar l}, k_{l\bar t}, \gamma)&\propto\sum_kM_{k}\delta(k_{lt}, k) + \gamma\delta(k_{lt}, \bar k)
\end{align*}
$$
Here, $M_k$ is the number of tables previously assigned to $\theta_k$, and $N_{jt}$ the number of customers already seated at the $t$th table in group $j$.