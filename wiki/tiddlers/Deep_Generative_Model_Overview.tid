created: 20161107062348968
modified: 20161107062355396
tags: [[Deep Generative Models]]
title: Deep Generative Model Overview
type: text/vnd.tiddlywiki

! Generative modeling
In real life we always encounter problems where data generator are so complicated that its distribution cannot be evaluated. Probably drawn from super high dimensional space (image/audio). Suppose we have trainig examples $x\sim p_{data}(x)$ and want a model that can draw samples: $x\sim p_{model}(x)$ where $p_{model} = p_{data}$. Of course this is the idealized scenario. 

Generally speaking, given high-dimensional data $X$, we want to estimate a low-dimensional model characterizing the population. Or consider [[Low-dimensionality manifold hypotheis]]

When given input of certain conditions, it can generate output with structure as rich as the input. This technique can be applied to speech synthesis, text translation etc. And it can generate data for other models, simulate the experiment environment and support reinforcement learning or simulate the reward of each action and use it in planning. We care this in:

* Simulation environments
* Prediction
* Inverse problems
* Transfer learning

Finally this might help us to leverage the unlabeled data problem. This possibility has not been demonstrated yet but we can assume the intermediate layers of GAN can be used as features for supervised tasks.

This task is challenging because we have to learn a representation from unlabeled data that captures regularity and complexity.

! Previous training methods
[[Survey of earlier work|http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf]]

People always train a generative model that represents the probability distribution. And fit this model using maximum likelihood. 

$$
\theta^* = \underset{\theta}{max}\frac 1 m\sum_{i=1}^m\log p(x^{(i)};\theta)
$$

Prior can be encoded in a parametric generatvie model with density estimation. [[GMM]] is a shallow model that assumes density concentrates in a finite number of modes. If data is sequential, we can exploit temporal regularity with [[Word2Vec]].

Until a few years ago the most popular way is to use undirected graphical models. And the flagship under the context of neural network is ''Deep Boltzmann machines'', where we define an energy function over the visible units and several hidden layers. We define an unnormalized probability distribution by taking $e$ to the negative energy fucntion, which has no constraint on it. And we normalize this positive functions to obtain a probability distribution function:

$$
\begin{align}
p(h, x) &= \frac 1 Z \tilde p(h, x)\\
\tilde p(h, x) &= \exp(-E(h, x))\\
Z&=\sum_{h, x}\tilde p(h, x)
\end{align}
$$

Unfortunately, computing that normalizing constant is NP-complete and our ability to approximate it is limited. In particular, if we train a Boltzmann machine well enough to represent the sharp distribution of widely separated modes, then our ability to approximate it decays. Our best approximation to the partition function is based on drawing samples from the model using ''MCMC''. And this Monte Carlo method works well as long as it is able to move between different modes. As the modes become sharper and better separated, we have a more difficult time transitioning between them. There are a lot of strategies to increase the temperature of the distribution temperarily in order to force the Markov chain to mix. But this problem is not solved to our satisfaction.

Some have been studying directed graphical models, where a similar problem arises. To compute the partition function, we need to sum over all configuartions of hidden states. ''Variational learning'' has been used to solve this. Where we maximize $\log p(x)-\mathcal D_{KL}(q(x)||p(z|x))$. And one of the most popular deep directed model is the [[Variational Autoencoder]]. The basic idea is we have a distribution of the hidden units $q(z)$, and we can draw samples from this distribution with a neural network and some noise. This idea of a differentiable decoder, a.k.a. a generative net is very wide spread and serveral approaches to train the generative decoder nets.

Another popular model is generative stochastic networks. These networks learn the transition probability of the Markov chains. Which are very similar to GANs. The basic strategy is to give up on having an explicit formula for $p(x)$, just learn to sample incrementally. The drawback is it is difficult for Markov chains to mix from one example to another extremely dissimilar example.