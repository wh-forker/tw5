created: 20160425022224634
modified: 20171012030714926
tags: 
title: NLP
type: text/vnd.tiddlywiki

[[Stanford NLP Course]]

* [[NLP Data]]

! Topics
* Perceiving and representing text: [[Word Embedding]]
** handling unknown words
** tokenize? spelling? characters? bytes?
** smaller perceptual units need more complex models, but have fewer OOVs (doubt)
* [[Text Classification]]
* [[Semantic Parsing]]
* Natual language generation
** [[Language Modelling]]
** [[Conditional Language Modelling]]
* Analytic applications
** topic modelling
** linguistic analysis (discourse, semantics, syntax, morphology)
* [[CNN for NLP]]

! Tools
* [[LSTMVis|https://github.com/HendrikStrobelt/LSTMVis]]

! Task & State-of-the-art techniques

* Question answering (babl): Strongly Supervised MemNN (Weston et al. 2015) 93.3%; Dynamic Memory Network 93.6%
* Sentiment Analysis (SST): Tree-LSTMs (Tai et al. 2015); DMN 88.5%
* Part of speech tagging (PTB-WSJ): Bi-directional LSTM-CRP (Huang et al. 2015); 

The performance is always dominated by the size of high quality data.

<<<
Probably the most successful concept is to use distributed representation of words. For example, neural network based language models significantly outperform N-gram models.
<<< [[Efficient Estimation of Word Representations in Vector Space|Word2Vec]]

! Sentence model
is to analysis and represent the semantic content of a sentence for purposes of classification or generation.

* sentiment analysis
* paraphrase detection
* entailment recognition
* summarisation
* discourse analysis
* grounded language learning
* image retrieval

!! Reviews
* Composition based methods for 
** vector representation of words or
** tied to particular syntactic relations or word types
* automatically extracted logical forms
* neural networks @@color:#d33682;
''advantages'': can be trained to obtain feature by predicting the context. can be powerful classifier/sentence generator
@@
** neural BoW or bag-of-//n//-grams
** recursive nets
** time-delay nets based on convolutional operations

