created: 20150213071121294
modified: 20170920073019890
tags: Analysis Statistics [[Reinforcement Learning]]
title: Fisher information metric
type: text/vnd.tiddlywiki

Fisher information metric is the only Riemannian metric that "makes sense" for [[statistical manifolds|Statistical manifold]]:
$$
I(\theta) = \left[\int\frac{\partial\log p(x;\theta)}{\partial\theta_i}\frac{\partial\log p(x;\theta)}{\partial\theta_j}p(x;\theta)dx\right]=[g_{ij}]
$$
The [[Infinitesimal length element]] is given by
$$
ds^2 = \sum_{i=1}^d\sum_{j=1}^dd\theta_i^T\nabla^2F(\theta)d\theta_j
$$

Cencov proved that for a non-singular transformation of the parameters $\lambda=f(\theta)$, the information matrix
$$
I(\lambda)=\left[\frac{\partial\theta_i}{\partial\lambda_j}\right]I(\theta)\left[\frac{\partial\theta_i}{\partial\lambda_j}\right].
$$

Normalized and de-correlated activation is well known for improving the conditioning of the Fisher information matrix.

! Appearances
In reinforcement learning, the natural gradient is defined as $F^{-1}g$, where $F$ is the average Fisher information matrix
$$
F = \mathbb E_{s, a\sim\pi}[(\nabla_\theta\log\pi_\theta(a|s))^T(\nabla_\theta\log\pi_\theta(a|s))]
$$