created: 20170621062553683
modified: 20180603105842534
tags: [[Probabilistic Deep Learning]] Bayesian
title: Bayesian Deep Learning
type: text/vnd.tiddlywiki

! History
First Bayesian NNs are training with Laplace's method. Neal uses HMC. In late 90s, VI is used.

Modern VI relied on a fully factorized approximation, assuming independence of each weight scalar in each layer from all other weights.

Expectation propagation is recently studied, relying on various forms of Renyi's $$\alpha$$-divergence. Seems to sacrifice their estimation of the dominant modes of the posterior.

! Introduction

Bayesian deep learning is

<<<
Posterior inference of layered representations of high-dimensional data
<<< Ranganath+, AISTATS 2015

Topics

* Deep exponential families: $$z_{n,l,k}\sim \text{Exp-Fam}(g(w^\top_{l,k}z_{n,l+1}))$$

! Bibs
* Recovering tractable distributions of weights
** ICML 2015 Weight uncertainty in neural networks
* Sampling based
** NIPS 2016 DISCO nets: DISsimilarity COefficient networks