created: 20170228070807064
modified: 20170503055003913
tags: [[ICLR 2017]] [[ICLR Paper Reviews]] Talks Meta-Learning
title: Optimization as a Model for Few-Shot Learning
type: text/vnd.tiddlywiki

! Reference

* Learning to learn by gradient descent by gradient descent

! Algorithm
Use LSTM to guide the classifier. SGD resembles the update for the cell state in an LSTM
$$
c_t = f_t\odot c_{t-1} + i_t\odot \tilde c_t
$$
we set cell state be the model parameters $c_{t-1} = \theta_{t-1}$, input gate asa learning rate, $i_t=\alpha_t$, and candidate cell state be the negative gradient $\tilde c_t = -\nabla_{\theta_{t-1}}\mathcal L_t$.

* Meta-training: train learner
* Meta-testing: train meta-learner and best initialization

# Use LSTM to model parameter dynamics during training
#* LSTM parameters are shares across $M$'s parameters. Learning an update rule applied to each parameter independently, but each parameter has its own history.
#* Learn $c_0$, like learning $M$'s initialization
# Inputs to meta-learning LSTM are the loss and gradient of learner
#* preprocess by Andrychowicz et al (2016) to ensure loss and gradient are at the same scale
# Train LSTM according to test sets
#* ignore gradients through the inputs of the LSTM.

! Tricks in training

Parameter sharing across all coordinates of the learner gradient, but each has its own hidden and cell states. This is implemented by batching the loss and gradient along there dimensions. @@color:#859900;Batch size should be really large.@@

* Gradients and losses are normalized the same way as in reference. 
* BP don't pass meta-learner.
* Init meta-learner's params small

! Remarks

There are works [[handling catastrophic forgetting|Overcoming catastrophic forgetting in NNs]]. Should think about it.

Resembles a sequential Gradient synthesizer.